version: '3.8'

services:
  # Redis for caching and session management
  redis:
    image: redis:7-alpine
    container_name: {{cookiecutter.project_slug}}-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # PostgreSQL database (conditional)
  {% if cookiecutter.include_database == "postgresql" %}
  postgres:
    image: postgres:15-alpine
    container_name: {{cookiecutter.project_slug}}-postgres
    environment:
      POSTGRES_DB: {{cookiecutter.project_slug}}
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "{{cookiecutter.postgres_port}}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backend/scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped
  {% endif %}

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: {{cookiecutter.project_slug}}-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zk_data:/var/lib/zookeeper/data
      - zk_logs:/var/lib/zookeeper/log
    restart: unless-stopped

  # Kafka for event streaming
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: {{cookiecutter.project_slug}}-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # RabbitMQ for message queuing
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: {{cookiecutter.project_slug}}-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    ports:
      - "5672:5672"   # AMQP port
      - "15672:15672" # Management UI
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 30s
      timeout: 30s
      retries: 3
    restart: unless-stopped

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: {{cookiecutter.project_slug}}-backend
    depends_on:
      redis:
        condition: service_healthy
      {% if cookiecutter.include_database == "postgresql" %}
      postgres:
        condition: service_healthy
      {% endif %}
      kafka:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    ports:
      - "{{cookiecutter.backend_port}}:{{cookiecutter.backend_port}}"
    environment:
      # Database configuration
      {% if cookiecutter.include_database == "postgresql" %}
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/{{cookiecutter.project_slug}}
      {% elif cookiecutter.include_database == "sqlite" %}
      DATABASE_URL: sqlite:///./data/{{cookiecutter.project_slug}}.db
      {% endif %}
      
      # Redis configuration
      REDIS_URL: redis://redis:6379/0
      
      # Celery configuration
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      
      # Kafka configuration
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      
      # RabbitMQ configuration
      RABBITMQ_URL: amqp://guest:guest@rabbitmq:5672/
      
      # LLM configuration
      {% if cookiecutter.llm_provider == "openai" %}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-your-openai-api-key-here}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4}
      {% elif cookiecutter.llm_provider == "anthropic" %}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-your-anthropic-api-key-here}
      ANTHROPIC_MODEL: ${ANTHROPIC_MODEL:-claude-3-sonnet-20240229}
      {% endif %}
      
      # App configuration
      ENVIRONMENT: development
      LOG_LEVEL: info
    volumes:
      - ./backend:/app
      {% if cookiecutter.include_database == "sqlite" %}
      - backend_data:/app/data
      {% endif %}
    restart: unless-stopped

  # Celery Worker - General Tasks
  celery-worker-general:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: {{cookiecutter.project_slug}}-celery-general
    depends_on:
      redis:
        condition: service_healthy
      {% if cookiecutter.include_database == "postgresql" %}
      postgres:
        condition: service_healthy
      {% endif %}
    command: celery -A app.core.celery_app:celery_app worker --queues=general --concurrency=2 --loglevel=info --hostname=general@%h
    environment:
      # Same environment as backend
      {% if cookiecutter.include_database == "postgresql" %}
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/{{cookiecutter.project_slug}}
      {% elif cookiecutter.include_database == "sqlite" %}
      DATABASE_URL: sqlite:///./data/{{cookiecutter.project_slug}}.db
      {% endif %}
      
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      RABBITMQ_URL: amqp://guest:guest@rabbitmq:5672/
      
      # LLM configuration
      {% if cookiecutter.llm_provider == "openai" %}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-your-openai-api-key-here}
      {% elif cookiecutter.llm_provider == "anthropic" %}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-your-anthropic-api-key-here}
      {% endif %}
      
      ENVIRONMENT: development
      LOG_LEVEL: info
    volumes:
      - ./backend:/app
      {% if cookiecutter.include_database == "sqlite" %}
      - backend_data:/app/data
      {% endif %}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "celery", "-A", "app.core.celery_app:celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Worker - Chat Tasks
  celery-worker-chat:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: {{cookiecutter.project_slug}}-celery-chat
    depends_on:
      redis:
        condition: service_healthy
      {% if cookiecutter.include_database == "postgresql" %}
      postgres:
        condition: service_healthy
      {% endif %}
    command: celery -A app.core.celery_app:celery_app worker --queues=chat --concurrency=3 --loglevel=info --hostname=chat@%h
    environment:
      # Same environment as backend
      {% if cookiecutter.include_database == "postgresql" %}
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/{{cookiecutter.project_slug}}
      {% elif cookiecutter.include_database == "sqlite" %}
      DATABASE_URL: sqlite:///./data/{{cookiecutter.project_slug}}.db
      {% endif %}
      
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      RABBITMQ_URL: amqp://guest:guest@rabbitmq:5672/
      
      # LLM configuration
      {% if cookiecutter.llm_provider == "openai" %}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-your-openai-api-key-here}
      {% elif cookiecutter.llm_provider == "anthropic" %}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-your-anthropic-api-key-here}
      {% endif %}
      
      ENVIRONMENT: development
      LOG_LEVEL: info
    volumes:
      - ./backend:/app
      {% if cookiecutter.include_database == "sqlite" %}
      - backend_data:/app/data
      {% endif %}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "celery", "-A", "app.core.celery_app:celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Worker - LLM Tasks (CPU intensive)
  celery-worker-llm:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: {{cookiecutter.project_slug}}-celery-llm
    depends_on:
      redis:
        condition: service_healthy
      {% if cookiecutter.include_database == "postgresql" %}
      postgres:
        condition: service_healthy
      {% endif %}
    command: celery -A app.core.celery_app:celery_app worker --queues=llm --concurrency=2 --loglevel=info --hostname=llm@%h --pool=prefork
    environment:
      # Same environment as backend
      {% if cookiecutter.include_database == "postgresql" %}
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/{{cookiecutter.project_slug}}
      {% elif cookiecutter.include_database == "sqlite" %}
      DATABASE_URL: sqlite:///./data/{{cookiecutter.project_slug}}.db
      {% endif %}
      
      REDIS_URL: redis://redis:6379/0
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/1
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      RABBITMQ_URL: amqp://guest:guest@rabbitmq:5672/
      
      # LLM configuration
      {% if cookiecutter.llm_provider == "openai" %}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-your-openai-api-key-here}
      {% elif cookiecutter.llm_provider == "anthropic" %}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-your-anthropic-api-key-here}
      {% endif %}
      
      ENVIRONMENT: development
      LOG_LEVEL: info
    volumes:
      - ./backend:/app
      {% if cookiecutter.include_database == "sqlite" %}
      - backend_data:/app/data
      {% endif %}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "celery", "-A", "app.core.celery_app:celery_app", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Flower - Monitoring (optional)
  celery-flower:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: {{cookiecutter.project_slug}}-flower
    depends_on:
      redis:
        condition: service_healthy
    command: celery -A app.core.celery_app:celery_app flower --port=5555
    ports:
      - "5555:5555"
    environment:
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/1
    restart: unless-stopped
    profiles:
      - monitoring

  # Next.js Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: {{cookiecutter.project_slug}}-frontend
    depends_on:
      - backend
    ports:
      - "{{cookiecutter.frontend_port}}:{{cookiecutter.frontend_port}}"
    environment:
      NEXT_PUBLIC_API_URL: http://localhost:{{cookiecutter.backend_port}}
      {% if cookiecutter.use_websockets == "yes" %}
      NEXT_PUBLIC_WS_URL: ws://localhost:{{cookiecutter.backend_port}}
      {% endif %}
      NEXT_PUBLIC_APP_NAME: "{{cookiecutter.project_name}}"
      NODE_ENV: development
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    restart: unless-stopped

  # Nginx reverse proxy (optional)
  nginx:
    image: nginx:alpine
    container_name: {{cookiecutter.project_slug}}-nginx
    depends_on:
      - frontend
      - backend
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped
    profiles:
      - production

volumes:
  redis_data:
  {% if cookiecutter.include_database == "postgresql" %}
  postgres_data:
  {% elif cookiecutter.include_database == "sqlite" %}
  backend_data:
  {% endif %}
  kafka_data:
  zk_data:
  zk_logs:
  rabbitmq_data:

networks:
  default:
    name: {{cookiecutter.project_slug}}-network
    driver: bridge
