# {{cookiecutter.project_name}} Backend

{{cookiecutter.description}}

A modern, production-ready FastAPI backend for AI-powered chat applications with comprehensive LLM support, real-time messaging, and enterprise-grade architecture.

## ğŸš€ Features

### Core Features
- **ğŸ¤– Multiple LLM Providers**: OpenAI, Anthropic, HuggingFace, and custom implementations
- **ğŸ’¬ Real-time Chat**: WebSocket support for instant messaging
- **ğŸ§  Conversation Memory**: Redis-based session storage with fallback to in-memory
- **ğŸ“Š Message Queuing**: Kafka and RabbitMQ integration for scalable event processing
- **ğŸ” Security**: JWT authentication, rate limiting, and comprehensive middleware
- **ğŸ“š Auto Documentation**: Interactive Swagger/ReDoc API documentation

### Architecture & Design
- **ğŸ—ï¸ Clean Architecture**: Layered design with clear separation of concerns
- **ğŸ”„ Dependency Injection**: Comprehensive DI container for all services
- **ğŸ­ Factory Pattern**: Flexible LLM provider switching
- **ğŸ“¦ Repository Pattern**: Abstracted data access with multiple backends
- **âš¡ Service Layer**: Business logic separation
- **ğŸ›¡ï¸ Exception Handling**: Comprehensive error management

### Production Ready
- **ğŸ³ Docker Support**: Multi-stage builds with development and production configurations
- **ğŸ“Š Health Checks**: Comprehensive service monitoring endpoints
- **ğŸ“ Structured Logging**: Advanced logging with request tracing
- **ğŸ”’ Security Headers**: CORS, security middleware, and rate limiting
- **ğŸ¯ Performance**: Optimized for high throughput and low latency

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”œâ”€â”€ config.py               # Application configuration
â”‚   â”œâ”€â”€ dependencies.py         # Dependency injection container
â”‚   â”œâ”€â”€ middleware.py           # Custom middleware stack
â”‚   â”œâ”€â”€ exceptions.py           # Exception handlers
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # API layer
â”‚   â”‚   â”œâ”€â”€ deps.py            # API-specific dependencies
â”‚   â”‚   â””â”€â”€ v1/                # API version 1
â”‚   â”‚       â”œâ”€â”€ router.py      # Main API router
â”‚   â”‚       â”œâ”€â”€ chat.py        # Chat endpoints
â”‚   â”‚       â”œâ”€â”€ completions.py # Text completion endpoints
â”‚   â”‚       â””â”€â”€ health.py      # Health check endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # Business logic core
â”‚   â”‚   â”œâ”€â”€ llm/               # LLM implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py        # Abstract LLM interface
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ huggingface_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ custom_client.py
â”‚   â”‚   â”‚   â””â”€â”€ factory.py     # LLM provider factory
â”‚   â”‚   â”œâ”€â”€ memory/            # Conversation memory
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py        # Memory interface
â”‚   â”‚   â”‚   â”œâ”€â”€ redis_memory.py
â”‚   â”‚   â”‚   â””â”€â”€ in_memory.py
â”‚   â”‚   â””â”€â”€ security/          # Security components
â”‚   â”‚       â”œâ”€â”€ auth.py        # Authentication
â”‚   â”‚       â””â”€â”€ rate_limit.py  # Rate limiting
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # Pydantic models
â”‚   â”‚   â”œâ”€â”€ chat.py            # Chat-related models
â”‚   â”‚   â”œâ”€â”€ completion.py      # Completion models
â”‚   â”‚   â””â”€â”€ base.py            # Base/common models
â”‚   â”‚
â”‚   â”œâ”€â”€ services/               # Business services
â”‚   â”‚   â”œâ”€â”€ chat_service.py    # Chat business logic
â”‚   â”‚   â”œâ”€â”€ completion_service.py
â”‚   â”‚   â”œâ”€â”€ conversation_service.py
â”‚   â”‚   â”œâ”€â”€ redis_client.py    # Redis integration
â”‚   â”‚   â”œâ”€â”€ kafka_client.py    # Kafka integration
â”‚   â”‚   â””â”€â”€ rabbitmq_client.py # RabbitMQ integration
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ logging.py         # Logging configuration
â”‚       â””â”€â”€ helpers.py         # Helper functions
```

## ğŸš¦ Quick Start

### Prerequisites

- **Python**: {{cookiecutter.python_version}}+
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **uv**: Latest (automatically installed in Docker)

### 1. Development Setup

```bash
# Start development environment
./scripts/start.sh development

# Check service status
./scripts/status.sh

# View logs
docker-compose -f docker/docker-compose.dev.yml logs -f backend-dev
```

### 2. Environment Configuration

Create a `.env` file based on `.env.example`:

```bash
cp .env.example .env
```

**Required Environment Variables:**

{% if cookiecutter.llm_provider == "openai" %}
```bash
# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4
```
{% elif cookiecutter.llm_provider == "anthropic" %}
```bash
# Anthropic Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229
```
{% endif %}

```bash
# Security (IMPORTANT: Change in production!)
SECRET_KEY=your-secret-key-change-in-production

# Database (if using PostgreSQL)
{% if cookiecutter.include_database == "postgresql" %}
DATABASE_URL=postgresql://postgres:postgres@localhost:{{cookiecutter.postgres_port}}/{{cookiecutter.project_slug}}
{% endif %}
```

### 3. Access the Application

Once started, the following services will be available:

| Service | URL | Description |
|---------|-----|-------------|
| **Backend API** | http://localhost:{{cookiecutter.backend_port}} | Main API server |
| **API Documentation** | http://localhost:{{cookiecutter.backend_port}}/docs | Interactive Swagger UI |
| **Health Check** | http://localhost:{{cookiecutter.backend_port}}/health | Service health status |
{% if cookiecutter.include_database == "postgresql" %}
| **pgAdmin** | http://localhost:5050 | Database management (admin@{{cookiecutter.project_slug}}.local / admin) |
{% endif %}
| **RabbitMQ Management** | http://localhost:15672 | Message queue management (guest/guest) |

## ğŸ³ Docker Usage

### Development

```bash
# Start all development services
./scripts/start.sh development

# Start with specific profiles (optional UIs)
docker-compose -f docker/docker-compose.dev.yml --profile kafka-ui --profile redis-ui up -d

# Stop services
./scripts/stop.sh development
```

### Production

```bash
# Deploy to production
./scripts/deploy.sh production

# Or start manually
./scripts/start.sh production

# Stop production services
./scripts/stop.sh production
```

## ğŸ“¡ API Usage

### Chat Endpoint

```bash
# Send a chat message
curl -X POST "http://localhost:{{cookiecutter.backend_port}}/api/v1/chat/" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello, how are you?",
    "session_id": "optional-session-id"
  }'
```

### Text Completion

```bash
# Generate text completion
curl -X POST "http://localhost:{{cookiecutter.backend_port}}/api/v1/completions/" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a short story about AI",
    "max_tokens": 200,
    "temperature": 0.8
  }'
```

### Health Check

```bash
# Check service health
curl "http://localhost:{{cookiecutter.backend_port}}/api/v1/health/"
```

## ğŸ”§ Configuration

### LLM Providers

The application supports multiple LLM providers:

{% if cookiecutter.llm_provider == "openai" %}
**OpenAI** (Current):
```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=your-key-here
OPENAI_MODEL=gpt-4
```
{% elif cookiecutter.llm_provider == "anthropic" %}
**Anthropic** (Current):
```bash
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=your-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229
```
{% elif cookiecutter.llm_provider == "huggingface" %}
**HuggingFace** (Current):
```bash
LLM_PROVIDER=huggingface
HUGGINGFACE_MODEL=microsoft/DialoGPT-medium
```
{% endif %}

**Switch Providers:**
```bash
# Change to different provider
LLM_PROVIDER=custom  # Uses mock responses for testing
```

### Memory Storage

**Redis** (Recommended):
```bash
REDIS_URL=redis://localhost:6379/0
```

**In-Memory** (Fallback):
- Automatically used when Redis is unavailable
- Development and testing only

### Database Configuration

{% if cookiecutter.include_database == "postgresql" %}
**PostgreSQL**:
```bash
DATABASE_URL=postgresql://user:pass@localhost:{{cookiecutter.postgres_port}}/{{cookiecutter.project_slug}}
```
{% elif cookiecutter.include_database == "sqlite" %}
**SQLite**:
```bash
DATABASE_URL=sqlite:///./data/{{cookiecutter.project_slug}}.db
```
{% else %}
**No Database**:
- Uses in-memory storage
- Configure DATABASE_URL if needed
{% endif %}

## ğŸ§ª Development

### Local Development (without Docker)

```bash
# Install uv (Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .

# Start the development server
uvicorn app.main:app --host 0.0.0.0 --port {{cookiecutter.backend_port}} --reload
```

### Running Tests

```bash
# Install test dependencies
uv pip install pytest pytest-asyncio httpx

# Run tests
pytest tests/

# With coverage
pytest tests/ --cov=app --cov-report=html
```

### Code Quality

```bash
# Install development tools
uv pip install black isort flake8 mypy

# Format code
black app/
isort app/

# Lint code
flake8 app/
mypy app/
```

## ğŸ“Š Monitoring

### Health Checks

The application provides comprehensive health checks:

- **`/health`**: Overall application health
- **`/api/v1/health/redis`**: Redis connectivity
- **`/api/v1/health/kafka`**: Kafka connectivity  
- **`/api/v1/health/rabbitmq`**: RabbitMQ connectivity
- **`/api/v1/health/ready`**: Kubernetes readiness probe
- **`/api/v1/health/live`**: Kubernetes liveness probe

### Logging

Structured logging with request tracing:

```python
# View logs
docker-compose -f docker/docker-compose.dev.yml logs -f backend-dev

# Filter by service
docker-compose logs -f redis-dev kafka-dev
```

### Metrics

Integration points for monitoring:

- **Prometheus**: Metrics endpoint (configure as needed)
- **Grafana**: Visualization (configure as needed)
- **Jaeger**: Distributed tracing (configure as needed)

## ğŸš€ Deployment

### Production Deployment

```bash
# Quick deployment
./scripts/deploy.sh production

# With custom settings
ENVIRONMENT=production \
SECRET_KEY=your-production-secret \
OPENAI_API_KEY=your-api-key \
./scripts/deploy.sh production
```

### Environment Variables for Production

**Required**:
- `SECRET_KEY`: Strong random secret key
- `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`: LLM provider credentials
- `POSTGRES_PASSWORD`: Database password

**Optional**:
- `CORS_ORIGINS`: Allowed origins for CORS
- `RATE_LIMIT_REQUESTS`: Requests per minute limit
- `LOG_LEVEL`: Logging level (INFO, WARNING, ERROR)

### Scaling

**Horizontal Scaling**:
```bash
# Scale backend instances
docker-compose -f docker/docker-compose.yml up --scale backend=3 -d
```

**Load Balancing**:
- Use Nginx reverse proxy (included)
- Configure SSL certificates
- Add health check monitoring

## ğŸ”’ Security

### Best Practices

1. **Environment Variables**: Never commit secrets to version control
2. **API Keys**: Use environment variables or secret management
3. **HTTPS**: Enable SSL/TLS in production (Nginx config provided)
4. **Rate Limiting**: Configure appropriate limits for your use case
5. **CORS**: Restrict origins to your domain
6. **Authentication**: Implement JWT or OAuth as needed

### Rate Limiting

```bash
# Configure rate limits
RATE_LIMIT_REQUESTS=100  # Requests per minute
RATE_LIMIT_WINDOW=60     # Time window in seconds
```

## ğŸ› Troubleshooting

### Common Issues

**Port Conflicts**:
```bash
# Check port usage
lsof -i :{{cookiecutter.backend_port}}
netstat -an | grep {{cookiecutter.backend_port}}
```

**Service Not Starting**:
```bash
# Check logs
docker-compose logs service-name

# Restart services
./scripts/stop.sh && ./scripts/start.sh
```

**Database Connection Issues**:
```bash
# Test database connection
docker exec {{cookiecutter.project_slug}}_postgres_dev pg_isready -U postgres

# Reset database
docker-compose down -v
docker-compose up -d
```

**LLM API Issues**:
- Verify API keys are set correctly
- Check API rate limits and quotas
- Test with `curl` commands
- Switch to custom provider for testing

### Debug Mode

```bash
# Enable debug mode
export DEBUG=true
export LOG_LEVEL=DEBUG

# Start with debug
./scripts/start.sh development
```

### Getting Help

1. **Check Logs**: `docker-compose logs -f backend-dev`
2. **Health Status**: `./scripts/status.sh`
3. **API Documentation**: http://localhost:{{cookiecutter.backend_port}}/docs
4. **Service Status**: http://localhost:{{cookiecutter.backend_port}}/health

## ğŸ“š API Documentation

Interactive API documentation is automatically generated and available at:

- **Swagger UI**: http://localhost:{{cookiecutter.backend_port}}/docs
- **ReDoc**: http://localhost:{{cookiecutter.backend_port}}/redoc
- **OpenAPI JSON**: http://localhost:{{cookiecutter.backend_port}}/openapi.json

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Commit** changes: `git commit -m 'Add amazing feature'`
4. **Push** to branch: `git push origin feature/amazing-feature`
5. **Open** a Pull Request

### Development Guidelines

- Follow PEP 8 style guide
- Add type hints for all functions
- Write comprehensive tests
- Update documentation
- Ensure Docker builds succeed

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¢ Support

- **Documentation**: Check `/docs` endpoint
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

---

**Generated by [cookiecutter-fastapi-nextjs-llm](https://github.com/your-org/cookiecutter-fastapi-nextjs-llm)**
