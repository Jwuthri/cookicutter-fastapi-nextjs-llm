# Production Docker Compose for {{cookiecutter.project_name}} Backend
version: '3.8'

networks:
  {{cookiecutter.project_slug}}_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  kafka_data:
  zookeeper_data:
  rabbitmq_data:

services:
  # Backend API Service
  backend:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: {{cookiecutter.project_slug}}_backend
    restart: unless-stopped
    ports:
      - "{{cookiecutter.backend_port}}:{{cookiecutter.backend_port}}"
    environment:
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=INFO
      - HOST=0.0.0.0
      - PORT={{cookiecutter.backend_port}}
      - WORKERS=4
      - RELOAD=false

      # Database
      {% if cookiecutter.include_database == "postgresql" %}
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD:-postgres}@postgres:5432/{{cookiecutter.project_slug}}
      {% endif %}

      # Redis
      - REDIS_URL=redis://redis:6379/0

      # Kafka
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_GROUP_ID={{cookiecutter.project_slug}}_backend_prod

      # RabbitMQ
      - RABBITMQ_URL=amqp://guest:${RABBITMQ_PASSWORD:-guest}@rabbitmq:5672/

      # LLM Configuration
      {% if cookiecutter.llm_provider == "openai" %}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      {% elif cookiecutter.llm_provider == "anthropic" %}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ANTHROPIC_MODEL=${ANTHROPIC_MODEL:-claude-3-sonnet-20240229}
      {% endif %}

      # Security
      - SECRET_KEY=${SECRET_KEY:-your-secret-key-change-in-production}
      - ACCESS_TOKEN_EXPIRE_MINUTES=${ACCESS_TOKEN_EXPIRE_MINUTES:-10080}

      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:{{cookiecutter.frontend_port}}}
    depends_on:
      {% if cookiecutter.include_database == "postgresql" %}
      - postgres
      {% endif %}
      - redis
      - kafka
      - rabbitmq
    networks:
      - {{cookiecutter.project_slug}}_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:{{cookiecutter.backend_port}}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  {% if cookiecutter.include_database == "postgresql" %}
  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: {{cookiecutter.project_slug}}_postgres
    restart: unless-stopped
    ports:
      - "{{cookiecutter.postgres_port}}:5432"
    environment:
      - POSTGRES_DB={{cookiecutter.project_slug}}
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - {{cookiecutter.project_slug}}_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
  {% endif %}

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: {{cookiecutter.project_slug}}_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - {{cookiecutter.project_slug}}_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru

  # Zookeeper for Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: {{cookiecutter.project_slug}}_zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    networks:
      - {{cookiecutter.project_slug}}_network
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka Message Broker
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: {{cookiecutter.project_slug}}_kafka
    restart: unless-stopped
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - {{cookiecutter.project_slug}}_network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5

  # RabbitMQ Message Queue
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    container_name: {{cookiecutter.project_slug}}_rabbitmq
    restart: unless-stopped
    ports:
      - "5672:5672"
      - "15672:15672"  # Management UI
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:-guest}
      RABBITMQ_DEFAULT_VHOST: /
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - {{cookiecutter.project_slug}}_network
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Nginx Reverse Proxy (Optional)
  nginx:
    image: nginx:alpine
    container_name: {{cookiecutter.project_slug}}_nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      # - ./ssl:/etc/nginx/ssl:ro  # Uncomment for SSL certificates
    depends_on:
      - backend
    networks:
      - {{cookiecutter.project_slug}}_network
    profiles:
      - nginx  # Optional service, start with: docker-compose --profile nginx up
