---
description: "LLM and AI agent integration patterns and best practices"
---

# LLM & AI Integration Rules

Follow these patterns for LLM and AI agent integration:

## LLM Service Architecture
- **Abstraction Layer**: Use LLM service abstractions in [app/core/llm/](mdc:{{cookiecutter.project_slug}}/backend/app/core/llm/)
- **Multi-Provider Support**: Support OpenRouter, OpenAI, Anthropic via factory pattern
- **Error Handling**: Implement robust error handling for external API failures
- **Rate Limiting**: Implement rate limiting to respect provider limits

## Memory Management
- **Memory Types**: Support in-memory, Redis, and vector database memory
- **Memory Configuration**: Configure via [app/core/memory/](mdc:{{cookiecutter.project_slug}}/backend/app/core/memory/)
- **Persistence**: Use persistent memory for important conversations
- **Cleanup**: Implement memory cleanup for expired sessions

Example memory configuration:
```python
# app/core/memory/memory_manager.py
class MemoryManager:
    def __init__(self, settings: Settings):
        if settings.memory_type == "vector":
            self.memory = VectorMemory(
                collection_name=settings.vector_collection_name,
                embeddings_model=settings.embeddings_model
            )
        elif settings.memory_type == "redis":
            self.memory = RedisMemory(
                redis_url=settings.redis_url
            )
        else:
            self.memory = InMemory()
```

## Chat Service Patterns

### Chat Service Implementation

