---
description: "LLM and AI agent integration patterns and best practices"
---

# LLM & AI Integration Rules

Follow these patterns for LLM and AI agent integration:

## LLM Service Architecture

### LangChain with OpenRouter
This project uses **LangChain** as the primary LLM framework, integrated with **OpenRouter** for unified access to 500+ language models.

- **Framework**: LangChain provides the abstraction layer for LLM interactions
- **Provider**: OpenRouter provides unified access to multiple model providers (OpenAI, Anthropic, Google, etc.)
- **Implementation**: Use `langchain_openai.ChatOpenAI` configured with OpenRouter's base URL
- **Abstraction Layer**: LLM service abstractions in [app/infrastructure/llm_provider.py](mdc:{{cookiecutter.project_slug}}/backend/app/infrastructure/llm_provider.py)
- **Error Handling**: Implement robust error handling for external API failures
- **Rate Limiting**: Implement rate limiting to respect provider limits

### OpenRouterProvider Usage
```python
from app.infrastructure.llm_provider import OpenRouterProvider
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Initialize provider
provider = OpenRouterProvider()

# Get LangChain LLM instance configured for OpenRouter
llm = provider.get_llm(
    model_name="openai/gpt-4o-mini",  # Any OpenRouter model
    temperature=0.7
)

# Create LangChain chain
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

chain = prompt | llm | StrOutputParser()

# Invoke chain
response = chain.invoke({"input": "Hello!"})
```

### Key Patterns
- **Use OpenRouterProvider**: Always use `OpenRouterProvider` to get LLM instances, not direct LangChain initialization
- **Model Names**: Use OpenRouter model format (e.g., `"openai/gpt-4o-mini"`, `"anthropic/claude-3-sonnet"`)
- **LangChain Chains**: Build chains using LangChain's pipe operator (`|`) for composability
- **Prompt Templates**: Use `ChatPromptTemplate` for structured prompts
- **Output Parsers**: Use appropriate output parsers (`StrOutputParser`, `JsonOutputParser`, etc.)

## Memory Management
- **Memory Types**: Support in-memory, Redis, and vector database memory
- **Memory Configuration**: Configure via [app/core/memory/](mdc:{{cookiecutter.project_slug}}/backend/app/core/memory/)
- **Persistence**: Use persistent memory for important conversations
- **Cleanup**: Implement memory cleanup for expired sessions

Example memory configuration:
```python
# app/core/memory/memory_manager.py
class MemoryManager:
    def __init__(self, settings: Settings):
        if settings.memory_type == "vector":
            self.memory = VectorMemory(
                collection_name=settings.vector_collection_name,
                embeddings_model=settings.embeddings_model
            )
        elif settings.memory_type == "redis":
            self.memory = RedisMemory(
                redis_url=settings.redis_url
            )
        else:
            self.memory = InMemory()
```

## Chat Service Patterns

### Chat Service Implementation
```python
from app.infrastructure.llm_provider import OpenRouterProvider
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

class ChatService:
    def __init__(self, memory_store, settings):
        self.provider = OpenRouterProvider()
        self.memory_store = memory_store
        self.settings = settings

    async def process_message(self, message: str, session_id: str, user_id: str = None):
        # Get conversation context from memory
        context = await self._get_conversation_context(session_id)

        # Get LLM instance
        llm = self.provider.get_llm(
            model_name=self.settings.default_model,
            temperature=0.7
        )

        # Create LangChain chain with context
        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful assistant."),
            *context,  # Include conversation history
            ("user", "{input}")
        ])

        chain = prompt | llm | StrOutputParser()

        # Generate response
        response = await chain.ainvoke({"input": message})

        # Store conversation
        await self._store_conversation(session_id, message, response)

        return {
            "message": response,
            "session_id": session_id,
            "message_id": generate_id(),
        }
```

## LangChain Tools & Agents

### Using LangChain Tools
```python
from app.infrastructure.llm_provider import OpenRouterProvider
from app.agents.tool.context_manager import count_history_items, get_recent_items

provider = OpenRouterProvider()
llm = provider.get_llm(model_name="openai/gpt-4o-mini")

# Bind tools to LLM
llm_with_tools = llm.bind_tools([count_history_items, get_recent_items])

# LLM can now use these tools
response = llm_with_tools.invoke("Count items in my history")
```

### Using LangChain Agents
```python
from app.agents.agents.context_manager import ContextManagerAgent
from app.infrastructure.llm_provider import OpenRouterProvider

provider = OpenRouterProvider()
agent = ContextManagerAgent(provider, model_name="openai/gpt-4o-mini")

# Use agent for context management
result = agent.check_context(request)
```

## Vector Database Integration
For memory and RAG (Retrieval-Augmented Generation):

### Supported Vector Databases
- **Pinecone**: Cloud-native vector database
- **Weaviate**: Open-source vector database
- **Qdrant**: High-performance vector database
- **ChromaDB**: Lightweight vector database

Example vector database setup:
```python
# For ChromaDB
from chromadb import Client
from chromadb.config import Settings as ChromaSettings

def create_chromadb_client(settings):
    return Client(ChromaSettings(
        persist_directory=settings.chromadb_path,
        anonymized_telemetry=False
    ))

# For Pinecone
import pinecone
from pinecone import Pinecone

def create_pinecone_client(settings):
    pc = Pinecone(api_key=settings.get_secret("pinecone_api_key"))
    return pc.Index(settings.pinecone_index_name)
```

## LLM Provider Configuration

### OpenRouter Integration
```python
from app.infrastructure.llm_provider import OpenRouterProvider

# Initialize provider (reads OPENROUTER_API_KEY from settings)
provider = OpenRouterProvider()

# Get LLM instance for any OpenRouter model
llm = provider.get_llm(
    model_name="openai/gpt-4o-mini",  # or "anthropic/claude-3-sonnet", etc.
    temperature=0.7,
    callbacks=[...]  # Optional LangChain callbacks
)

# The provider automatically:
# - Sets base_url to OpenRouter API
# - Uses your OpenRouter API key
# - Supports all 500+ models available on OpenRouter
```

### Model Selection Guidelines
- **Fast Responses**: Use `"openai/gpt-4o-mini"` or `"openai/gpt-4o"` for quick responses
- **Complex Reasoning**: Use `"anthropic/claude-3-sonnet"` or `"openai/gpt-4"` for complex tasks
- **Code Generation**: Use specialized code models when available
- **Cost Optimization**: Balance model capability with cost requirements
- **Model Format**: Always use OpenRouter format: `"provider/model-name"`

## Error Handling & Resilience

### Retry Logic with LangChain
```python
from tenacity import retry, stop_after_attempt, wait_exponential
from app.infrastructure.llm_provider import OpenRouterProvider

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def call_llm_with_retry(prompt: str, model: str):
    try:
        provider = OpenRouterProvider()
        llm = provider.get_llm(model_name=model)
        response = await llm.ainvoke(prompt)
        return response
    except Exception as e:
        logger.warning(f"LLM call failed: {e}")
        raise
```

### Fallback Strategies
- **Model Fallback**: Try cheaper/faster models if primary fails
- **Provider Fallback**: Switch providers on failures
- **Cache Fallback**: Use cached responses when available
- **Graceful Degradation**: Provide meaningful error messages to users

## Prompt Engineering

### LangChain Prompt Templates
```python
from langchain_core.prompts import ChatPromptTemplate

SYSTEM_PROMPTS = {
    "helpful_assistant": ChatPromptTemplate.from_messages([
        ("system", "You are a helpful, harmless, and honest AI assistant. Provide accurate and helpful responses while being concise and clear."),
        ("user", "{input}")
    ]),

    "code_assistant": ChatPromptTemplate.from_messages([
        ("system", "You are an expert software developer. Provide high-quality code solutions with explanations. Follow best practices and include error handling."),
        ("user", "{input}")
    ]),

    "customer_support": ChatPromptTemplate.from_messages([
        ("system", "You are a customer support agent. Be empathetic, professional, and solution-oriented. Escalate complex issues when necessary."),
        ("user", "{input}")
    ])
}
```

### Context Management
- **Context Limits**: Respect model context window limits (use `get_model_limits()` utility)
- **Context Compression**: Summarize older messages when needed
- **Relevance Filtering**: Include only relevant conversation history
- **Metadata Preservation**: Maintain important conversation metadata
- **Token Counting**: Use `count_tokens_llm()` utility to track token usage

## Security & Privacy

### Data Handling
- **Data Minimization**: Only send necessary data to LLM providers
- **PII Scrubbing**: Remove or mask personally identifiable information
- **Input Sanitization**: Clean user inputs before sending to LLMs
- **Output Filtering**: Filter potentially harmful LLM responses

### API Key Management
- **Secret Storage**: Store API keys in secure secret management (environment variables)
- **Rotation**: Implement API key rotation procedures
- **Monitoring**: Monitor API usage and costs via OpenRouter dashboard
- **Rate Limiting**: Implement usage limits per user/session

## Monitoring & Observability

### LLM Metrics
- **Response Times**: Track latency for different models
- **Token Usage**: Monitor input/output token consumption (use `count_tokens_llm()`)
- **Success Rates**: Track API success/failure rates
- **Cost Tracking**: Monitor costs per request and per user via OpenRouter

### Logging
```python
from app.utils.logging import get_logger

logger = get_logger("llm_service")

logger.info(
    "LLM request completed",
    extra={
        "model": model_name,
        "tokens_used": token_count,
        "response_time": response_time,
        "user_id": user_id,
        "session_id": session_id
    }
)
```

## Testing LLM Integration

### Mock LLM Services
```python
from unittest.mock import Mock, AsyncMock
from langchain_core.messages import AIMessage

class MockLLMService:
    def __init__(self, responses: List[str] = None):
        self.responses = responses or ["Mock response"]
        self.call_count = 0

    async def ainvoke(self, prompt, **kwargs):
        response = self.responses[self.call_count % len(self.responses)]
        self.call_count += 1
        return AIMessage(content=response)
```

### Integration Testing
- **Mock External APIs**: Never call real LLM APIs in tests
- **Response Validation**: Test response format validation
- **Error Scenarios**: Test various API failure scenarios
- **Performance Testing**: Test with realistic token loads
- **Chain Testing**: Test LangChain chains end-to-end with mocked LLMs
